{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae161c39",
   "metadata": {},
   "source": [
    "## PG Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e36a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from src.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14c36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageGraphDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PageGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'graph-metadata.csv'\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # Used in `self.process`\n",
    "        self.data = self.__get_graph_metadata(filter_nas=True, uq=0.99, uc=0.99).reset_index()\n",
    "\n",
    "        return [f'data_{ix}.pt' for ix in self.data.index.to_list()]\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # Get node and edge types across all graphs\n",
    "        graph_ids_with_labels = self.data['graph_id_with_label'].to_list()\n",
    "        node_types_set, edge_types_set = self.__generate_sets_of_edge_and_node_types(graph_ids_with_labels)\n",
    "        \n",
    "        # Fit encoders\n",
    "        node_types_np = np.array(list(node_types_set)).reshape(-1, 1)\n",
    "        node_type_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        node_type_enc.fit(node_types_np)\n",
    "\n",
    "        edge_types_np = np.array(list(edge_types_set)).reshape(-1, 1)\n",
    "        edge_type_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        edge_type_enc.fit(edge_types_np)\n",
    "        \n",
    "        for (ix, row) in tqdm(self.data.iterrows()):\n",
    "            G = self.__load_relabelled_graph(row['graph_id_with_label'])\n",
    "\n",
    "            node_types_oe = self.__encode_node_features(G, node_type_enc)\n",
    "            edge_types_oe = self.__encode_edge_features(G, edge_type_enc)\n",
    "            edge_index = self.__encode_edge_index(G)\n",
    "                \n",
    "            _, label = row['graph_id_with_label'].split('-')\n",
    "            data = Data(x=node_types_oe, edge_index=edge_index, edge_attr=edge_types_oe, y=torch.tensor(int(label), dtype=torch.int64), graph_id=row['graph_id_with_label'])\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{ix}.pt'))\n",
    "\n",
    "            \n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "            \n",
    "\n",
    "    def get(self, graph_id):\n",
    "        return torch.load(os.path.join(self.processed_dir, f'data_{graph_id}.pt'))\n",
    "            \n",
    "\n",
    "    def __get_subdirectory(self, label):\n",
    "        return BROKEN_DIR if bool(int(label)) else UNBROKEN_DIR\n",
    "\n",
    "\n",
    "    def __get_graph_path(self, graph_id_with_label, graph_type):\n",
    "        assert(isinstance(graph_type, GraphType))\n",
    "\n",
    "        graph_id, label = graph_id_with_label.split('-')\n",
    "        if graph_type == GraphType.DELTA:\n",
    "            path = DELTA_PATH\n",
    "        elif graph_type == GraphType.PRE_INTERVENTION:\n",
    "            path = PRE_INTERVENTION_PATH\n",
    "        elif graph_type == GraphType.POST_INTERVENTION:\n",
    "            path = POST_INTERVENTION_PATH\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return os.path.join(self.raw_dir, self.__get_subdirectory(label), graph_id, path)\n",
    "\n",
    "\n",
    "    def __get_graph_metadata(self, filter_nas=False, iqr=False, uq=False, uc=1):\n",
    "        meta_df = pd.read_csv(self.raw_paths[0], converters={'G_pre_cols': pd.eval})\n",
    "\n",
    "        if filter_nas:\n",
    "            meta_df = meta_df[meta_df[['G_pre_nodes', 'G_delta_nodes']].isna().sum(axis=1) == 0]\n",
    "\n",
    "        if not (iqr or uq):\n",
    "            return meta_df\n",
    "\n",
    "        if iqr:\n",
    "            assert(0 < iqr < 1)\n",
    "            eps = (1-iqr)/2\n",
    "            lower_quantile, upper_quantil = 0+eps, 1-eps\n",
    "\n",
    "            lb_nodes = meta_df['G_pre_nodes'].quantile(lower_quantile)\n",
    "            ub_nodes = meta_df['G_pre_nodes'].quantile(upper_quantil)\n",
    "            lb_edges = meta_df['G_pre_edges'].quantile(lower_quantile)\n",
    "            ub_edges = meta_df['G_pre_edges'].quantile(upper_quantil)\n",
    "\n",
    "            predicate = \"\"\"\n",
    "                G_pre_nodes >= %i and \\\n",
    "                G_pre_nodes <=%i and \\\n",
    "                G_pre_edges >= %i and \\\n",
    "                G_pre_edges <=%i\n",
    "            \"\"\" % (lb_edges, ub_edges, lb_nodes, ub_nodes)\n",
    "            return meta_df.query(predicate)\n",
    "\n",
    "        if uq:\n",
    "            assert(0 < uq < 1)\n",
    "            meta_df['edge_node_ratio'] = meta_df['G_pre_edges'] / meta_df['G_pre_nodes']\n",
    "            ub_edge_node_ratio = meta_df['edge_node_ratio'].quantile(uq)\n",
    "\n",
    "            assert(0 < uc <= 1)\n",
    "            ub_node_count = meta_df['G_pre_nodes'].quantile(uc)\n",
    "            ub_edge_count = meta_df['G_pre_edges'].quantile(uc)\n",
    "\n",
    "            predicate = \"\"\"\n",
    "                edge_node_ratio <= %i and \\\n",
    "                G_pre_nodes <= %i and \\\n",
    "                G_pre_edges <= %i\n",
    "            \"\"\" % (ub_edge_node_ratio, ub_node_count, ub_edge_count)\n",
    "            return meta_df.query(predicate)\n",
    "    \n",
    "\n",
    "    def __load_relabelled_graph(self, graph_id_with_label):\n",
    "        graph_path = self.__get_graph_path(graph_id_with_label, GraphType.DELTA)\n",
    "        G = nx.read_graphml(graph_path)\n",
    "        return nx.convert_node_labels_to_integers(G)\n",
    "\n",
    "\n",
    "    def __generate_sets_of_edge_and_node_types(self, ixs):\n",
    "        node_types_set = set()\n",
    "        edge_types_set = set()\n",
    "\n",
    "        for graph_id in tqdm(ixs):\n",
    "            G = self.__load_relabelled_graph(graph_id)\n",
    "\n",
    "            graph_node_types_ser = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')['node type'].unique()\n",
    "            node_types_set = node_types_set.union(set(graph_node_types_ser))\n",
    "\n",
    "            edge_node_types_ser = nx.to_pandas_edgelist(G)['edge type']\n",
    "            edge_types_set = edge_types_set.union(set(edge_node_types_ser))\n",
    "\n",
    "        return node_types_set, edge_types_set\n",
    "\n",
    "    \n",
    "    def __encode_node_features(self, G, enc):\n",
    "        node_types_ser = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')['node type']\n",
    "        node_types_np = node_types_ser.to_numpy().reshape(-1, 1)\n",
    "        return torch.tensor(enc.transform(node_types_np).toarray(), dtype=torch.float)\n",
    "\n",
    "\n",
    "    def __encode_edge_features(self, G, enc):\n",
    "        edge_types_ser = nx.to_pandas_edgelist(G)['edge type']\n",
    "        edge_types_np = edge_types_ser.to_numpy().reshape(-1, 1)\n",
    "        return torch.tensor(enc.transform(edge_types_np).toarray(), dtype=torch.float)\n",
    "\n",
    "    \n",
    "    def __encode_edge_index(self, G):\n",
    "        # edges = list(G.edges(keys=False))\n",
    "        edges = list(G.edges())\n",
    "        return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "dataset = PageGraphDataset('/Volumes/brave-build-drive/pg-gnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf421ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PageGraphDataset(1924)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf7b78",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadf31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Sigmoid\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import (GCNConv, GATConv, GATv2Conv, TopKPooling,\n",
    "                                global_mean_pool, global_max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dd0cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 24, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "num_node_features = dataset[0].x.shape[1]\n",
    "num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "num_conv_layers = 3-2\n",
    "num_heads = 1\n",
    "\n",
    "num_node_features, num_edge_features, num_conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "771ddc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # torch.manual_seed(0)\n",
    "        self.initial_conv = GATv2Conv(num_node_features, embedding_size, heads=num_heads, edge_dim=num_edge_features)\n",
    "\n",
    "        self.module_list = torch.nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            conv = GATv2Conv(embedding_size, embedding_size, heads=num_heads, edge_dim=num_edge_features)\n",
    "            self.module_list.append(conv)\n",
    "\n",
    "        self.final_conv = GATv2Conv(embedding_size, embedding_size, heads=num_heads, edge_dim=num_edge_features)\n",
    "        \n",
    "        self.lin = Linear(embedding_size*1, 1)\n",
    "        self.sig = Sigmoid()\n",
    "        \n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.initial_conv(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        \n",
    "        for conv in self.module_list:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = x.relu()\n",
    "        \n",
    "        x = self.final_conv(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # x = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return self.sig(self.lin(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5b99de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GATv2Conv(16, 64, heads=1)\n",
      "  (module_list): ModuleList(\n",
      "    (0): GATv2Conv(64, 64, heads=1)\n",
      "  )\n",
      "  (final_conv): GATv2Conv(64, 64, heads=1)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Number of parameters:  23873\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "# 23873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2f96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data PageGraphDataset(1539)\n",
      "loss tensor(0.6886) correct 370179.0\n",
      "loss tensor(0.6947) correct 83545.0\n",
      "loss tensor(0.6910) correct 369670.0\n",
      "loss tensor(0.6938) correct 83545.0\n",
      "loss tensor(0.6948) correct 369670.0\n",
      "loss tensor(0.6935) correct 83545.0\n",
      "loss tensor(0.7034) correct 369161.0\n",
      "loss tensor(0.6938) correct 83545.0\n",
      "loss tensor(0.6970) correct 369670.0\n",
      "loss tensor(0.6944) correct 83545.0\n",
      "loss tensor(0.6859) correct 370179.0\n",
      "loss tensor(0.6944) correct 83545.0\n",
      "loss tensor(0.6990) correct 369670.0\n",
      "loss tensor(0.6940) correct 83545.0\n",
      "loss tensor(0.7031) correct 369670.0\n",
      "loss tensor(0.6936) correct 83545.0\n",
      "loss tensor(0.6892) correct 370179.0\n",
      "loss tensor(0.6937) correct 83545.0\n",
      "loss tensor(0.6806) correct 370688.0\n",
      "loss tensor(0.6940) correct 83545.0\n",
      "Epoch: 010, Loss: 0.6741, Train Loss: 0.6806, Test Loss: 0.6940, Train Acc: 370688.0000, Test Acc: 83545.0000\n",
      "loss tensor(0.6871) correct 370179.0\n",
      "loss tensor(0.6949) correct 83545.0\n",
      "loss tensor(0.6816) correct 370179.0\n",
      "loss tensor(0.6958) correct 83545.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m losses, train_losses, test_losses, train_accs, test_accs \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     64\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m         data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch, data\u001b[38;5;241m.\u001b[39medge_attr)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:616\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    618\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch_geometric/data/dataset.py:197\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 197\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mPageGraphDataset.get\u001b[0;34m(self, graph_id)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_id):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgraph_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/serialization.py:705\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    707\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1/lib/python3.10/site-packages/torch/serialization.py:242\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "        targets = data.y.unsqueeze(1).to(torch.float32)\n",
    "        loss = criterion(out, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch, data.edge_attr)  \n",
    "            targets = data.y.unsqueeze(1).to(torch.float32)\n",
    "            loss = criterion(out, targets)\n",
    "            correct += (out.argmax(1) == targets).type(torch.float).sum().item()\n",
    "\n",
    "    # loss /= len(loader)\n",
    "    # correct /= len(loader.dataset)\n",
    "    \n",
    "    print('loss', loss, 'correct', correct)\n",
    "\n",
    "    return loss, correct\n",
    "        \n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "data_size = len(dataset)\n",
    "NUM_GRAPHS_PER_BATCH = 512\n",
    "\n",
    "print('data', dataset[:int(data_size * 0.8)])\n",
    "\n",
    "train_loader = DataLoader(dataset[:int(data_size * 0.8)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(data_size * 0.8):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "losses, train_losses, test_losses, train_accs, test_accs = [], [], [], [], []\n",
    "for epoch in range(1, 1000):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    \n",
    "    train_loss, train_acc = test(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    test_loss, test_acc = test(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "end_time = time.time()\n",
    "duration = end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677d8c9-f2d9-4ffd-894a-24d8181841e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "losses_float = [float(loss.cpu().detach().numpy()) for loss in losses]\n",
    "loss_ixs = [i for i,l in enumerate(losses_float)]\n",
    "\n",
    "sns.lineplot(loss_ixs, losses_float, label='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404d967-067d-4a0a-b4ed-5fe47eedb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(loss_ixs, train_accs, label='train')\n",
    "sns.lineplot(loss_ixs, test_accs, label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e12df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.007)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43654542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data in a data loader\n",
    "data_size = len(dataset)\n",
    "NUM_GRAPHS_PER_BATCH = 128\n",
    "\n",
    "loader = DataLoader(dataset[:int(data_size * 0.6)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(data_size * 0.6):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "# for batch in loader:\n",
    "#     x = torch.Tensor(np.vstack(batch.x)).float()\n",
    "#     print(x.shape)\n",
    "#     print(batch.batch.shape)\n",
    "#     print(batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58873d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Enumerate over the data\n",
    "    for batch in loader:\n",
    "        # Use GPU\n",
    "        batch.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Passing the node features and the connection info\n",
    "#         x = torch.Tensor(np.vstack(batch.x)).float()\n",
    "\n",
    "        pred, embedding = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(pred, batch.y.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # Update using the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss, embedding\n",
    "\n",
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "for epoch in range(25):\n",
    "    loss, h = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning (training loss)\n",
    "import seaborn as sns\n",
    "losses_float = [float(loss.cpu().detach().numpy()) for loss in losses] \n",
    "loss_indices = [i for i,l in enumerate(losses_float)] \n",
    "plt = sns.lineplot(loss_indices, losses_float)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Analyze the results for one batch\n",
    "test_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    test_batch.to(device)\n",
    "    pred, embed = model(test_batch.x, test_batch.edge_index, test_batch.batch) \n",
    "    df = pd.DataFrame()\n",
    "    df[\"y_real\"] = test_batch.y.tolist()\n",
    "    df[\"y_pred\"] = pred.tolist()\n",
    "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row)\n",
    "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de12ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.scatterplot(data=df, x=\"y_real\", y=\"y_pred\")\n",
    "plt.set(xlim=(-7, 2))\n",
    "plt.set(ylim=(-7, 2))\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a636b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
